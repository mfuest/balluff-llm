# Balluff-LLM: Fine-Tuning LLMs for Edge Deployment

## Overview
This project explores the **fine-tuning and deployment of Large Language Models (LLMs) on resource-constrained edge devices**, specifically **Raspberry Pi 5 (4GB & 8GB RAM)**, for **industrial applications at Balluff**. 

We fine-tuned **TinyLlama**, a lightweight model, using **Low-Rank Adaptation (LoRA)** to optimize it for **on-device inference**. The primary goal was to **enable privacy-preserving LLM deployments** without relying on cloud services, making it viable for **industrial environments with strict data regulations**.

## Features
- âœ… **Fine-tuning**: Applied **LoRA-based PEFT** to efficiently adapt TinyLlama with minimal resources.
- âœ… **Quantization**: Optimized the model for edge deployment using **BitsAndBytes**.
- âœ… **Raspberry Pi 5 Deployment**: Evaluated feasibility on both **4GB and 8GB** versions.
- âœ… **Industrial Use Case**: Trained on **Balluff product-specific data** for real-world industrial applications.
- âœ… **Performance Evaluations**: Benchmarked response speed, accuracy, and feasibility.

---

## Project Structure

ğŸ“¦ balluff-llm
â”£ ğŸ“‚ 4GB_rpi5_LLM/      # TinyLlama fine-tuned model & checkpoints
â”£ ğŸ“‚ data/              # Preprocessed Balluff product data
â”£ ğŸ“œ RAG.ipynb          # Alternative RAG-based approach (not used in final version)
â”£ ğŸ“œ README.md          # This file
â”£ ğŸ“œ .gitignore         # Git ignore rules
â”— ğŸ“œ Data_Balluff.xlsx  # Sample dataset (if applicable)

---

## ğŸ› ï¸ Installation & Setup
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/mfuest/balluff-llm.git
cd balluff-llm

2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

3ï¸âƒ£ Run the Model on Raspberry Pi

python run_model.py --model_path 4GB_rpi5_LLM/tinyllama-finetuned-mps/



â¸»

ğŸ—ï¸ Fine-Tuning Process

Model Selection

We initially tested Mistral 7B and TinyLlama, but due to memory constraints on Raspberry Pi 5, we focused on TinyLlama.

Data Preprocessing
	â€¢	Extracted Balluff product data (PDF datasheets)
	â€¢	Converted to structured context-prompt-response format
	â€¢	Cleaned and normalized data using ChatGPT-assisted preprocessing

Fine-Tuning Approach
	â€¢	LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning
	â€¢	Training performed on Google Colab (A100, T4 GPUs)
	â€¢	Final model uploaded to Hugging Face: Final Model

â¸»

ğŸ Results & Performance
	â€¢	Response Speed: ~13 tokens/sec
	â€¢	Validation Accuracy: ~75% correct responses on industrial test cases
	â€¢	Memory Usage: 2.2GB fine-tuned model (fits within Raspberry Pi 5 constraints)
	â€¢	Trade-offs: LLM struggles with numerical data consistency (e.g., IP addresses)

â¸»

ğŸ”„ Alternative Approaches: Retrieval-Augmented Generation (RAG)

We also explored RAG (Retrieval-Augmented Generation) but found:
	â€¢	Faster inference (~49 tokens/sec)
	â€¢	Lower accuracy (0% correct responses in test cases)
	â€¢	Higher adaptability (no fine-tuning required)

Ultimately, we chose fine-tuning over RAG due to better domain-specific accuracy.

â¸»

ğŸ“Œ Limitations & Future Work
	â€¢	ğŸš§ Data Augmentation: More training data could improve accuracy
	â€¢	ğŸš§ Model Compression: Further quantization to reduce RAM footprint
	â€¢	ğŸš§ Hybrid Approach: Combining RAG + fine-tuning for dynamic queries
	â€¢	ğŸš§ Multi-device Optimization: Scaling to other edge devices beyond Raspberry Pi

â¸»

ğŸ‘¨â€ğŸ’» Contributors
	â€¢	Maximilian Fuest â€“ Raspberry Pi LLM testing, fine-tuning code, report writing
	â€¢	Yufei Xu â€“ RAG implementation, evaluation, report writing
	â€¢	Yusuf A. GÃ¼n â€“ Data preprocessing, fine-tuning, model training, validation

â¸»

ğŸ“œ References
	â€¢	TUM x Balluff AI Project Report
	â€¢	Fine-Tuned Model on Hugging Face
### **Next Steps**
âœ… **Copy this `README.md` to your GitHub repo**  
âœ… **Update any missing details (e.g., dataset access, additional dependencies)**  
âœ… **Add installation & usage details based on your actual code structure**  

Let me know if you'd like any modifications! ğŸš€
